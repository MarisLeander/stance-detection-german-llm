{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-22T13:28:14.523638Z",
     "start_time": "2025-05-22T13:28:14.427070Z"
    }
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:28:40.903709Z",
     "start_time": "2025-05-22T13:28:40.896872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def request_data(collection_name:str, offset:int=0) -> requests.Response:\n",
    "    \"\"\"\n",
    "    Function to request data from the Bundestag website.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): The name of the collection to request data from. (e.g. 866354-866354)\n",
    "        offset (int): The offset for pagination. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "        requests.Response: The response object from the request.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the URL for the request\n",
    "    url = f\"https://www.bundestag.de/ajax/filterlist/de/services/opendata/{collection_name}?limit=10&noFilterSet=true&offset={offset}\"\n",
    "\n",
    "    # Set the headers for the request\n",
    "    headers = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "        \"Referer\": \"https://www.bundestag.de/services/opendata\",\n",
    "        \"Sec-Fetch-Dest\": \"empty\",\n",
    "        \"Sec-Fetch-Mode\": \"cors\",\n",
    "        \"Sec-Fetch-Site\": \"same-origin\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Mobile Safari/537.36\",\n",
    "        \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "    }\n",
    "\n",
    "    # Make the request to the Bundestag website\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.ok:\n",
    "        return response\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def parse_response(response:requests.Response) -> list[dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Function to parse the response from the Bundestag website. All single entries of parliamentary minutes, denoted by a <tr> tag are stored in a list.\n",
    "\n",
    "    Args:\n",
    "        response (requests.Response): The response object from the request.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the title, link, and description of each plenary minute.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the response content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    rows = soup.find_all('tr')\n",
    "    # List to hold data\n",
    "    documents = []\n",
    "\n",
    "    for row in rows:\n",
    "        # Get the title\n",
    "        title_tag = row.select_one('td[data-th=\"Titel\"] div.bt-documents-description p strong')\n",
    "        title = title_tag.text.strip() if title_tag else \"No title found\"\n",
    "\n",
    "        # Get the link to the XML document\n",
    "        link_tag = row.select_one('td[data-th=\"Titel\"] ul.bt-linkliste li a.bt-link-dokument')\n",
    "        link = link_tag['href'] if link_tag else \"No link found\"\n",
    "\n",
    "        # Optional: Extract additional info such as file size\n",
    "        description = link_tag.text.strip() if link_tag else \"No description\"\n",
    "\n",
    "        # Append to documents list\n",
    "        documents.append({\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'description': description\n",
    "        })\n",
    "\n",
    "    return documents\n",
    "\n",
    "def write_json(data:list[dict[str, str]], filename:str):\n",
    "    \"\"\"\n",
    "    Convert the data list to JSON and save it to a file\n",
    "\n",
    "    Args:\n",
    "        data (list): List of documents to save.\n",
    "        filename (str): Name of the file to save the data to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(f'{filename}', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Documents successfully saved to {filename}\")\n",
    "\n",
    "\n",
    "def scrape_collection(file_name:str, collection_name:str):\n",
    "    \"\"\"\n",
    "    Function to scrape a collection of parliamentary minutes from the Bundestag website.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): The name of the collection to scrape data from. (e.g. 866354-866354)\n",
    "        file_name (str): The name of the file to save the data to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to hold all documents\n",
    "    all_documents = []\n",
    "    # Initialize offset for pagination\n",
    "    offset = 0\n",
    "\n",
    "    while True:\n",
    "        response =  request_data(collection_name, offset)\n",
    "\n",
    "        if response is None:\n",
    "            break # Exit if the request failed\n",
    "        else:\n",
    "            # Parse the response and extract documents\n",
    "            documents = parse_response(response)\n",
    "            all_documents.extend(documents)\n",
    "            # Check if there are more documents to fetch\n",
    "            if len(documents) < 10:\n",
    "                break # Exit if there are no more documents\n",
    "            else:\n",
    "                offset += 10 # Increment the offset for the next request\n",
    "\n",
    "    # Save the data to a json file\n",
    "    write_json(all_documents, file_name)\n"
   ],
   "id": "14460e3cff283729",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:29:21.761063Z",
     "start_time": "2025-05-22T13:29:14.497412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"Main function to scrape all plenary minutes from the Bundestag website. It scrapes the data from the collections defined in the files dictionary and saves them to JSON files.\"\"\"\n",
    "    # All plenary minutes are stored in collections. The minutes from the previous legislative periods are can be found as a zip on the Bundestag website (https://www.bundestag.de/services/opendata)\n",
    "    files = {1058442: \"Plenarprotokolle_21_wahlperiode\", 866354: \"Plenarprotokolle_20_wahlperiode\", 543410: \"Plenarprotokolle_19_wahlperiode\"}\n",
    "\n",
    "    # Loop through each collection and scrape the data\n",
    "    for key, value in files.items():\n",
    "        # Create a unique collection name for each collection. This is important for the HTTP request to the Bundestag website.\n",
    "        collection_name = f\"{key}-{key}\"\n",
    "        filename = f\"{value}.json\"\n",
    "        print(f\"Scraping collection {filename}...\")\n",
    "        scrape_collection(filename, collection_name)\n",
    "        print(f\"Finished scraping collection {filename}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "8e09c2b831a1164a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping collection Plenarprotokolle_21_wahlperiode.json...\n",
      "Documents successfully saved to Plenarprotokolle_21_wahlperiode.json\n",
      "Finished scraping collection Plenarprotokolle_21_wahlperiode.json.\n",
      "Scraping collection Plenarprotokolle_20_wahlperiode.json...\n",
      "Documents successfully saved to Plenarprotokolle_20_wahlperiode.json\n",
      "Finished scraping collection Plenarprotokolle_20_wahlperiode.json.\n",
      "Scraping collection Plenarprotokolle_19_wahlperiode.json...\n",
      "Documents successfully saved to Plenarprotokolle_19_wahlperiode.json\n",
      "Finished scraping collection Plenarprotokolle_19_wahlperiode.json.\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
