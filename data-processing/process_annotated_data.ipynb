{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:48:07.725259Z",
     "start_time": "2025-07-01T09:48:07.721835Z"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb as db\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0655fdee310b638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:53:54.562653Z",
     "start_time": "2025-07-01T09:53:54.557898Z"
    }
   },
   "outputs": [],
   "source": [
    "def connect_to_db(db_path: str) -> db.DuckDBPyConnection:\n",
    "    \"\"\"\n",
    "    Connect to a DuckDB database.\n",
    "\n",
    "    Args:\n",
    "        db_path (str): The path to the DuckDB database file.\n",
    "\n",
    "    Returns:\n",
    "        duckdb.DuckDBPyConnection: A connection object to the DuckDB database.\n",
    "    \"\"\"\n",
    "    return db.connect(database=db_path, read_only=False)\n",
    "\n",
    "def create_tables(con:db.DuckDBPyConnection, reset_db:bool=False) -> None:\n",
    "    if reset_db:\n",
    "        con.execute(\"DROP TABLE IF EXISTS annotations\")\n",
    "        con.execute(\"DROP TABLE IF EXISTS annotated_paragraphs\")\n",
    "\n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS annotated_paragraphs (\n",
    "            id INTEGER PRIMARY KEY REFERENCES group_mention(id),\n",
    "            group_text VARCHAR NOT NULL,\n",
    "            inference_paragraph VARCHAR NOT NULL,\n",
    "            adjusted_span BOOLEAN NOT NULL\n",
    "            );\n",
    "    \"\"\"\n",
    "    con.execute(sql)\n",
    "\n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS annotations (\n",
    "            id INTEGER NOT NULL,\n",
    "            annotator VARCHAR(32) NOT NULL,\n",
    "            annotated_paragraph_id INTEGER NOT NULL REFERENCES annotated_paragraphs(id),\n",
    "            stance VARCHAR(8) CHECK (stance IN ('favour', 'against', 'neither', 'not a group')),\n",
    "            PRIMARY KEY(id, annotator)\n",
    "        );\n",
    "    \"\"\"\n",
    "    con.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8be388a25a49e3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:53:54.749556Z",
     "start_time": "2025-07-01T09:53:54.744609Z"
    }
   },
   "outputs": [],
   "source": [
    "def adjust_inference_para(paragraph:str, group_span_adjustment:list[dict]) -> str:\n",
    "    # remove the span tags\n",
    "    clean_paragraph = paragraph.replace(\"<span>\", \"\").replace(\"</span>\", \"\")\n",
    "    # get placement of adjusted group span\n",
    "\n",
    "    offsets = group_span_adjustment[0].get('globalOffsets')\n",
    "    start = offsets.get(\"start\")\n",
    "    end = offsets.get(\"end\")\n",
    "\n",
    "\n",
    "    # input validation\n",
    "    # ensures start and end are valid integers and in the correct order.\n",
    "    if not (isinstance(start, int) and isinstance(end, int) and 0 <= start <= end <= len(clean_paragraph)):\n",
    "        print(f\"Warning: Invalid offsets provided. start={start}, end={end}, text_length={len(clean_paragraph)}\")\n",
    "        # Return the original text if offsets are invalid to prevent errors.\n",
    "        return None\n",
    "\n",
    "    # slice string, based on offsets to build new inference paragraph\n",
    "    before_span = clean_paragraph[:start]\n",
    "    span_content = clean_paragraph[start:end]\n",
    "    after_span = clean_paragraph[end:]\n",
    "\n",
    "    # construct the new string with the tags wrapped around the middle part.\n",
    "    return f\"{before_span}<span>{span_content}</span>{after_span}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe8562d8bb0b6660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:53:54.947351Z",
     "start_time": "2025-07-01T09:53:54.941884Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_primary_annotations(path:str, con:db.DuckDBPyConnection) -> None:\n",
    "    \"\"\" Processes annotations from the primary annotator. This will be used, to\n",
    "        build our annotated_paragraphs table. That means only the inference paragraphs are used / adjusted.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the annotation file.\n",
    "        con (db.DuckDBPyConnection): the database connection\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Read annotated data\n",
    "    annotation_data = pd.read_csv(path)\n",
    "    # Iterate over each annotated entry and add it to db\n",
    "    for index, row in annotation_data.iterrows():\n",
    "        group_text = row['group_text']\n",
    "        paragraph_id = row['id']\n",
    "        inference_paragraph = row['inference_paragraph']\n",
    "        group_span_adjustments = row['answer']\n",
    "        adjusted_span = False\n",
    "\n",
    "        if not pd.isna(group_span_adjustments):\n",
    "            adjusted_span = True\n",
    "            # Convert list string into list\n",
    "            group_span_adjustments = json.loads(group_span_adjustments)\n",
    "            # If our adjustments are not NA it means, that the group span was adjusted!\n",
    "            inference_paragraph = adjust_inference_para(inference_paragraph, group_span_adjustments)\n",
    "            # Adjust group text, since the group span was adjusted\n",
    "            group_text = group_span_adjustments[0].get('text')\n",
    "            \n",
    "        # Insert annotated paragraph into db\n",
    "        sql = \"\"\"\n",
    "            INSERT INTO annotated_paragraphs (id, group_text, inference_paragraph, adjusted_span)\n",
    "                 VALUES (?, ?, ?, ?)\n",
    "              \"\"\"\n",
    "        con.execute(sql, (paragraph_id, group_text, inference_paragraph, adjusted_span))\n",
    "\n",
    "def process_annotations(path:str, annotator:str, con:db.DuckDBPyConnection) -> None:\n",
    "    \"\"\" Processes annotations from an annotator. This will be used, to fill the annotations table, where annotations from each annotator are stored\n",
    "    \n",
    "    Args:\n",
    "        path (str): The path to the annotation file.\n",
    "        annotator (str): The name of the annotator\n",
    "        con (db.DuckDBPyConnection): The database connection\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Read annotated data\n",
    "    annotation_data = pd.read_csv(path)\n",
    "\n",
    "    for index, row in annotation_data.iterrows():\n",
    "        id = index\n",
    "        annotator = annotator.lower()\n",
    "        paragraph_id = row['id']\n",
    "        stance = row['stance_annotation'].lower()\n",
    "        sql = \"INSERT INTO annotations (id, annotator, annotated_paragraph_id, stance) VALUES (?, ?, ?, ?);\"\n",
    "        con.execute(sql, (id, annotator, paragraph_id, stance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e9865000ebba20a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:07:10.405572Z",
     "start_time": "2025-07-01T08:07:10.312493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing of annotations...\n",
      "Finished processing\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\" Main function to process annotated data.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get path to db\n",
    "    home_dir = Path.home()\n",
    "    db_path = home_dir / \"stance-detection-german-llm\" / \"data\" / \"database\" / \"german-parliament.duckdb\"\n",
    "    # Get db connection\n",
    "    con = connect_to_db(db_path)\n",
    "\n",
    "\n",
    "\n",
    "    # # build argparser, to pass information whether to reset db or not when starting the script\n",
    "    # parser = argparse.ArgumentParser(description=\"Process annotated data.\")\n",
    "\n",
    "    # parser.add_argument(\n",
    "    #     \"--reset_db\",\n",
    "    #     action='store_true', # This is the key change\n",
    "    #     help=\"If this flag is present, the annotations table will be reset.\"\n",
    "    # )\n",
    "    # args = parser.parse_args()\n",
    "    \n",
    "    con.begin()\n",
    "    # create_tables(con, reset_db=args.reset_db)\n",
    "    create_tables(con, reset_db=True)\n",
    "    con.commit()\n",
    "    print(\"Start processing of annotations...\")\n",
    "    con.begin()\n",
    "    path =  home_dir / \"stance-detection-german-llm\" / \"data\" / \"annotated_data\"/ \"maris-2025-06-30-14-57-790f3829.csv\"\n",
    "    # ---- Build annotated_paragraphs table ----\n",
    "    process_primary_annotations(path, con)\n",
    "    con.commit()\n",
    "\n",
    "    # --- Build annotations table for each annotator ---\n",
    "    annotator_files = [(\"maris-2025-06-30-14-57-790f3829.csv\", \"maris\"),('harriet_tmp-2025-07-01-13-33-c16b5183.csv', 'harriet_tmp')]\n",
    "\n",
    "    for file, name in annotator_files:\n",
    "        path =  home_dir / \"stance-detection-german-llm\" / \"data\" / \"annotated_data\"/ \"maris-2025-06-30-14-57-790f3829.csv\"\n",
    "        con.begin()\n",
    "        process_annotations(path, name, con)\n",
    "        con.commit()\n",
    "        \n",
    "    con.close()\n",
    "    print(\"Finished processing\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15ee4e43402bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:07:10.471887Z",
     "start_time": "2025-07-01T08:07:10.470365Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991b41be97b3d029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
