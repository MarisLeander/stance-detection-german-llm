{
 "cells": [
  {
   "cell_type": "code",
   "id": "20bc4d60-4174-4a55-8886-f9cea1d8a8b3",
   "metadata": {},
   "source": [
    "import duckdb\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "# Import classifier\n",
    "#from ipynb.fs.full.group_classifier import predict_batch\n",
    "from group_classifier import predict_batch\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "# Disables progress bar of .map function\n",
    "disable_progress_bar()\n",
    "print(\"Import successful!!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0e9c2acf-64d4-4cf4-a816-c9315fef11f5",
   "metadata": {},
   "source": [
    " # Connect to sql database\n",
    "con = duckdb.connect(database='../data/database/german-parliament.duckdb', read_only=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "87a49673-3f68-4987-a75c-4ec16e6ed945",
   "metadata": {},
   "source": [
    "def preprocess_speech(speech: tuple) -> tuple[int, list[dict[str, str]]]:\n",
    "    \"\"\"The filtering for skipping_president_remarks is only necessare for periods >= 19 because of the \"new\" format provided by the bundestag. \n",
    "    For periods <19 the method just extracs all the 'p' tags\n",
    "\n",
    "    Args:\n",
    "        speech (tuple): A tuple containing the speech data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the speech ID and a list of paragraphs with their text content.\n",
    "    \"\"\"\n",
    "    speech_id = speech[0]\n",
    "    text_content = speech[5]\n",
    "\n",
    "    # Parse xml from string will throw ParseError if not parseable\n",
    "    root = ET.fromstring(text_content)\n",
    "    \n",
    "    paragraphs_text = []\n",
    "    # skipping_president_remarks is used to detect interruptions of the President\n",
    "    skipping_president_remarks = False\n",
    "\n",
    "    # Iterate over all direct children of the root <rede> element\n",
    "    for element in root:\n",
    "        # 1. Check if we need to STOP skipping\n",
    "        if skipping_president_remarks:\n",
    "            if element.tag == 'p' and element.attrib.get(\"klasse\") == \"redner\":\n",
    "                # We know that the interruption of the president ended and the speech continues after this tag\n",
    "                skipping_president_remarks = False\n",
    "                continue \n",
    "            elif element.tag == 'p':\n",
    "                # Do nothing -> president is speaking\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Detected unexpected tag: {element.tag}\")\n",
    "\n",
    "        # 2. Check if we need to START skipping (President speaks)\n",
    "        if element.tag == 'name':\n",
    "            # Check if the text content of the name tag indicates a presiding\n",
    "            name_text = \"\".join(element.itertext()).strip() # Gets all text within <name>, including children\n",
    "            # State all titles which will get ignored\n",
    "            president_titles = [\"Pr채sidentin\", \"Pr채sident\", \"Vizepr채sidentin\", \"Vizepr채sident\"]\n",
    "            if name_text and any(title in name_text for title in president_titles):\n",
    "                skipping_president_remarks = True\n",
    "                continue # Move to the next element, don't process this <name> tag as a paragraph\n",
    "\n",
    "        # 3. Process <p> tags\n",
    "        if element.tag == 'p':\n",
    "            # Filter out speaker information paragraphs\n",
    "            if element.attrib.get(\"klasse\") == \"redner\":\n",
    "                continue\n",
    "            else:\n",
    "                # Actual text content of speech!\n",
    "                # Get text of the paragraph and remove potential irrelevant whitespaces\n",
    "                p_text = element.text.strip() if element.text else \"\"\n",
    "                \n",
    "                item = {\"paragraphs\": p_text}\n",
    "                paragraphs_text.append(item)\n",
    "        \n",
    "        # Other tags like <kommentar> are implicitly skipped as they are not 'p'\n",
    "\n",
    "    return (speech_id, paragraphs_text)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be7eee91013bc3dc",
   "metadata": {},
   "source": [
    "def create_paragraphs_classified_table(reset_db:bool=False):\n",
    "    \"\"\"\n",
    "    Creates a table for classified paragraphs in the database.\n",
    "\n",
    "    Args:\n",
    "        reset_db (bool): If True, drops the table if it exists before creating it.\n",
    "                         Defaults to False.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if reset_db:\n",
    "        con.execute(\"DROP TABLE IF EXISTS group_mention\")\n",
    "        con.execute(\"DROP SEQUENCE IF EXISTS group_mention_id_seq\")\n",
    "\n",
    "    # Create a sequence for the primary key\n",
    "    con.execute(\"CREATE SEQUENCE IF NOT EXISTS group_mention_id_seq START 1;\")\n",
    "\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS group_mention (\n",
    "            id INTEGER DEFAULT nextval('group_mention_id_seq') PRIMARY KEY,\n",
    "            paragraph_no INTEGER, -- If its 0, its the first paragraph of the speech, 1 for the second, etc.\n",
    "            speech_id VARCHAR NOT NULL REFERENCES speech(id),\n",
    "            paragraph VARCHAR NOT NULL,\n",
    "            group_text VARCHAR NOT NULL, -- This is the group mention, e.g. die Mitglieder der SPD-Fraktion\n",
    "            label VARCHAR(15) NOT NULL,\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def smart_join(tokens):\n",
    "    \"\"\"\n",
    "    Joins a list of word tokens into a single string, handling punctuation\n",
    "    and sub-word prefixes ('##') correctly.\n",
    "\n",
    "    Args:\n",
    "        tokens (list[str]): A list of word tokens, which may include sub-word tokens prefixed with '##'.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string with tokens joined together, ensuring proper spacing around punctuation.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # Define punctuation that should not have a preceding space\n",
    "    no_space_before = {',', '.', '?', '!', ';', ':', ')'}\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        # If it's the very first token, a punctuation mark, or a sub-word,\n",
    "        # don't add a leading space.\n",
    "        if i > 0 and token not in no_space_before and not token.startswith('##'):\n",
    "            result.append(' ')\n",
    "\n",
    "        # Append the token itself, removing any '##' prefixes\n",
    "        result.append(token.replace('##', ''))\n",
    "\n",
    "    return \"\".join(result).replace(' - ', '-') # Removes the space before and after a hyphen (Bindestrich)\n",
    "\n",
    "def extract_groups(paragraph:list[tuple[str,str]]) -> list[tuple[str, str]]:\n",
    "    \"\"\" Extracts group mention along with their labels from a paragraph. It groups tokens by their entity labels to get the full mention.\n",
    "    If a mention is broken e.g it does not start with a 'B-' label, it will be filtered.\n",
    "\n",
    "    Args:\n",
    "        paragraph (list[tuple[str,str]]): A list of tuples containing tokens and their corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[str, str]]: A list of tuples where each tuple contains the entity label and a list of (token, label) pairs for that entity, which contain the full mention.\n",
    "    \"\"\"\n",
    "    # This is a set of special tokens that should be ignored in the grouping process. -> adjust it if necessary\n",
    "    SPECIAL = {\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[UNK]\"}\n",
    "    # empty list for groups of paragraph\n",
    "    groups = []\n",
    "    # This is a temporary list to hold the current group mention\n",
    "    group_tmp = []\n",
    "    # This is a flag to indicate if we are currently inside a group mention\n",
    "    group_started = False\n",
    "    entity = \"\" # This hold the current entity. e.g. EOPOL for B-EOPOL\n",
    "    for token, label in paragraph:\n",
    "        # If token is a special token like [CLS] skip it\n",
    "        if token in SPECIAL:\n",
    "            continue\n",
    "        # Check for begin of group\n",
    "        elif label.startswith(\"B-\"):\n",
    "            group_started = True\n",
    "            entity = label[2:]\n",
    "            if group_tmp:\n",
    "                groups.append((entity, group_tmp))\n",
    "                group_tmp = [] # New Group will begin\n",
    "            # Append new beginning label\n",
    "            group_tmp.append((token, label))\n",
    "        # It is checked that 1) we have an inside label and 2) There was a B- label before!\n",
    "        elif label.startswith(\"I-\") and group_started:\n",
    "            # Then we check if the entity matches\n",
    "            if label[2:] != entity:\n",
    "                # print(f\"Current label: {label[2:]} doesn't match beginning label: {entity}\") @todo handle this as error log\n",
    "                # Break current group because of the miss-label\n",
    "                group_started = False\n",
    "            else:\n",
    "            # If all tests hold, we append the token and its label to the current group\n",
    "                group_tmp.append((token, label))\n",
    "        elif label == 'O':\n",
    "            # An 'O' Tag is always outside. Thus if we scan one, it means that the current group is over\n",
    "            if group_tmp:\n",
    "                groups.append((entity, group_tmp))\n",
    "                group_tmp = [] # New Group will begin\n",
    "            group_started = False\n",
    "        else:\n",
    "            pass\n",
    "            # print(f\"Filtered faulty classification: ({token}, {label})\") @todo handle this as error log\n",
    "\n",
    "    # Flush last word\n",
    "    if group_tmp:\n",
    "        groups.append((entity, group_tmp))\n",
    "\n",
    "    return groups\n",
    "\n",
    "def insert_paragraph(speech_id:int, index:int, entity:str, group_clean_text:str, paragraph:str):\n",
    "    \"\"\"\n",
    "    Inserts a classified paragraph into the database.\n",
    "\n",
    "    Args:\n",
    "        speech_id (int): The ID of the speech.\n",
    "        index (int): The index of the paragraph in the speech. 0 for the first paragraph, 1 for the second, etc.\n",
    "        entity (str): The entity label of the paragraph. For example, 'EOPOL' for B-EOPOL.\n",
    "        group_clean_text (str): The cleaned text of the paragraph.\n",
    "        paragraph (str): The original paragraph text.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    con.execute(\"\"\"\n",
    "        INSERT INTO group_mention (paragraph_no, speech_id, paragraph, group_text, label)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "        ON CONFLICT DO NOTHING; -- If the paragraph already exists, do nothing\n",
    "    \"\"\", (index, speech_id, paragraph, group_clean_text, entity))\n",
    "    con.commit()\n",
    "\n",
    "def insert_group_mention(speech_id:str, index:int, groups:list[tuple[str,list[tuple[str,str]]]], paragraph:str):\n",
    "    \"\"\"Inserts classified paragraphs into the database.\n",
    "\n",
    "    Args:\n",
    "        speech_id (str): The ID of the speech.\n",
    "        index (int): The index of the paragraph in the speech. 0 for the first paragraph, 1 for the second, etc.\n",
    "        groups (list[tuple[str,list[tuple[str,str]]]]): A list of tuples containing the entity and a list of the token, label pairs for each group.\n",
    "        paragraph (str): The original paragraph text.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for group in groups:\n",
    "        entity, raw_tokens = group\n",
    "        tokens = [item[0] for item in raw_tokens]\n",
    "        group_clean_text = smart_join(tokens)\n",
    "        # print(f\"{entity} -> {group_clean_text}\")\n",
    "        insert_paragraph(speech_id, index, entity, group_clean_text, paragraph)\n",
    "\n",
    "\n",
    "def process_speech(speech_id:str, paragraphs:list[dict[str, list[str]]]):\n",
    "    \"\"\"Processes a speech by classifying its paragraphs (extracting group mention) and inserting them into the database.\n",
    "\n",
    "    Args:\n",
    "        speech_id (str): The ID of the speech.\n",
    "        paragraphs (list[dict[str, list[str]]]): The list of paragraphs, each represented as a dictionary with a 'paragraphs' key containing the text.\n",
    "    \"\"\"\n",
    "    group_mention = predict_batch(paragraphs)\n",
    "    for index, p in enumerate(group_mention):\n",
    "        # print(p)\n",
    "        groups = extract_groups(p)\n",
    "        insert_group_mention(speech_id, index, groups, paragraphs[index].get('paragraphs'))\n",
    "\n",
    "def extract_speech() -> tuple:\n",
    "    \"\"\"Extracts a random speech from the database that has not been processed yet.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the speech data, including its ID, title, date, and text content.\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "        SELECT *\n",
    "        FROM speech\n",
    "        WHERE position NOT IN ('Pr채sidentin', 'Vizepr채sidentin', 'Vizepr채sident', 'Pr채sident')\n",
    "              AND id NOT IN (SELECT speech_id FROM group_mention) -- check that speech wasn't already processed\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "    return con.execute(sql).fetchone()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Main function to process speeches and extract group mentions.\n",
    "    \"\"\"\n",
    "    create_paragraphs_classified_table(reset_db=True)\n",
    "    for i in tqdm(range(500), desc='Processing:'):\n",
    "        speech = extract_speech()\n",
    "        speech_id, paragraphs = preprocess_speech(speech)\n",
    "        process_speech(speech_id, paragraphs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "f07d0a096dc1c114",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
