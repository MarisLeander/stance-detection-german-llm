{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20bc4d60-4174-4a55-8886-f9cea1d8a8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 08:25:58.926862: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-08 08:26:02.064146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749363963.132240  418349 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749363963.514071  418349 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-08 08:26:06.127970: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful!!\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "# Import classifier\n",
    "#from ipynb.fs.full.group_classifier import predict_batch\n",
    "from group_classifier import predict_batch\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "# Disables progress bar of .map function\n",
    "disable_progress_bar()\n",
    "print(\"Import successful!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e9c2acf-64d4-4cf4-a816-c9315fef11f5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Connect to sql database\n",
    "con = duckdb.connect(database='../data/database/german-parliament.duckdb', read_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a49673-3f68-4987-a75c-4ec16e6ed945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_speech(speech: tuple) -> tuple[int, list[dict[str, str]]]:\n",
    "    \"\"\"The filtering for skipping_president_remarks is only necessare for periods >= 19 because of the \"new\" format provided by the bundestag. \n",
    "    For periods <19 the method just extracs all the 'p' tags\n",
    "\n",
    "    Args:\n",
    "        speech (tuple): A tuple containing the speech data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the speech ID and a list of paragraphs with their text content.\n",
    "    \"\"\"\n",
    "    speech_id = speech[0]\n",
    "    text_content = speech[5]\n",
    "\n",
    "    # Parse xml from string will throw ParseError if not parseable\n",
    "    root = ET.fromstring(text_content)\n",
    "    \n",
    "    paragraphs_text = []\n",
    "    # skipping_president_remarks is used to detect interruptions of the President\n",
    "    skipping_president_remarks = False\n",
    "\n",
    "    # Iterate over all direct children of the root <rede> element\n",
    "    for element in root:\n",
    "        # 1. Check if we need to STOP skipping\n",
    "        if skipping_president_remarks:\n",
    "            if element.tag == 'p' and element.attrib.get(\"klasse\") == \"redner\":\n",
    "                # We know that the interruption of the president ended and the speech continues after this tag\n",
    "                skipping_president_remarks = False\n",
    "                continue \n",
    "            elif element.tag == 'p':\n",
    "                # Do nothing -> president is speaking\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Detected unexpected tag: {element.tag}\")\n",
    "\n",
    "        # 2. Check if we need to START skipping (President speaks)\n",
    "        if element.tag == 'name':\n",
    "            # Check if the text content of the name tag indicates a presiding\n",
    "            name_text = \"\".join(element.itertext()).strip() # Gets all text within <name>, including children\n",
    "            # State all titles which will get ignored\n",
    "            president_titles = [\"Präsidentin\", \"Präsident\", \"Vizepräsidentin\", \"Vizepräsident\"]\n",
    "            if name_text and any(title in name_text for title in president_titles):\n",
    "                skipping_president_remarks = True\n",
    "                continue # Move to the next element, don't process this <name> tag as a paragraph\n",
    "\n",
    "        # 3. Process <p> tags\n",
    "        if element.tag == 'p':\n",
    "            # Filter out speaker information paragraphs\n",
    "            if element.attrib.get(\"klasse\") == \"redner\":\n",
    "                continue\n",
    "            else:\n",
    "                # Actual text content of speech!\n",
    "                # Get text of the paragraph and remove potential irrelevant whitespaces\n",
    "                p_text = element.text.strip() if element.text else \"\"\n",
    "                \n",
    "                item = {\"paragraphs\": p_text}\n",
    "                paragraphs_text.append(item)\n",
    "        \n",
    "        # Other tags like <kommentar> are implicitly skipped as they are not 'p'\n",
    "\n",
    "    return (speech_id, paragraphs_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7eee91013bc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paragraphs_classified_table(reset_db:bool=False):\n",
    "    \"\"\"\n",
    "    Creates a table for classified paragraphs in the database.\n",
    "\n",
    "    Args:\n",
    "        reset_db (bool): If True, drops the table if it exists before creating it.\n",
    "                         Defaults to False.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if reset_db:\n",
    "        con.execute(\"DROP TABLE IF EXISTS group_mention\")\n",
    "        con.execute(\"DROP SEQUENCE IF EXISTS group_mention_id_seq\")\n",
    "\n",
    "    # Create a sequence for the primary key\n",
    "    con.execute(\"CREATE SEQUENCE IF NOT EXISTS group_mention_id_seq START 1;\")\n",
    "\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS group_mention (\n",
    "            id INTEGER DEFAULT nextval('group_mention_id_seq') PRIMARY KEY,\n",
    "            paragraph_no INTEGER, -- If its 0, its the first paragraph of the speech, 1 for the second, etc.\n",
    "            speech_id VARCHAR NOT NULL REFERENCES speech(id),\n",
    "            paragraph VARCHAR NOT NULL,\n",
    "            group_text VARCHAR NOT NULL, -- This is the group mention, e.g. die Mitglieder der SPD-Fraktion\n",
    "            label VARCHAR(15) NOT NULL,\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f07d0a096dc1c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_join(tokens):\n",
    "    \"\"\"\n",
    "    Joins a list of word tokens into a single string, handling punctuation\n",
    "    and sub-word prefixes ('##') correctly.\n",
    "\n",
    "    Args:\n",
    "        tokens (list[str]): A list of word tokens, which may include sub-word tokens prefixed with '##'.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string with tokens joined together, ensuring proper spacing around punctuation.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # Define punctuation that should not have a preceding space\n",
    "    no_space_before = {',', '.', '?', '!', ';', ':', ')'}\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        # If it's the very first token, a punctuation mark, or a sub-word,\n",
    "        # don't add a leading space.\n",
    "        if i > 0 and token not in no_space_before and not token.startswith('##'):\n",
    "            result.append(' ')\n",
    "\n",
    "        # Append the token itself, removing any '##' prefixes\n",
    "        result.append(token.replace('##', ''))\n",
    "\n",
    "    return \"\".join(result).replace(' - ', '-') # Removes the space before and after a hyphen (Bindestrich)\n",
    "\n",
    "def extract_groups(paragraph:list[tuple[str,str]]) -> list[tuple[str, str]]:\n",
    "    \"\"\" Extracts group mention along with their labels from a paragraph. It groups tokens by their entity labels to get the full mention.\n",
    "    If a mention is broken e.g it does not start with a 'B-' label, it will be filtered.\n",
    "\n",
    "    Args:\n",
    "        paragraph (list[tuple[str,str]]): A list of tuples containing tokens and their corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[str, str]]: A list of tuples where each tuple contains the entity label and a list of (token, label) pairs for that entity, which contain the full mention.\n",
    "    \"\"\"\n",
    "    # This is a set of special tokens that should be ignored in the grouping process. -> adjust it if necessary\n",
    "    SPECIAL = {\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[UNK]\"}\n",
    "    # empty list for groups of paragraph\n",
    "    groups = []\n",
    "    # This is a temporary list to hold the current group mention\n",
    "    group_tmp = []\n",
    "    # This is a flag to indicate if we are currently inside a group mention\n",
    "    group_started = False\n",
    "    entity = \"\" # This hold the current entity. e.g. EOPOL for B-EOPOL\n",
    "    for token, label in paragraph:\n",
    "        # If token is a special token like [CLS] skip it\n",
    "        if token in SPECIAL:\n",
    "            continue\n",
    "        # Check for begin of group\n",
    "        elif label.startswith(\"B-\"):\n",
    "            group_started = True\n",
    "            entity = label[2:]\n",
    "            if group_tmp:\n",
    "                groups.append((entity, group_tmp))\n",
    "                group_tmp = [] # New Group will begin\n",
    "            # Append new beginning label\n",
    "            group_tmp.append((token, label))\n",
    "        # It is checked that 1) we have an inside label and 2) There was a B- label before!\n",
    "        elif label.startswith(\"I-\") and group_started:\n",
    "            # Then we check if the entity matches\n",
    "            if label[2:] != entity:\n",
    "                # print(f\"Current label: {label[2:]} doesn't match beginning label: {entity}\") @todo handle this as error log\n",
    "                # Break current group because of the miss-label\n",
    "                group_started = False\n",
    "            else:\n",
    "            # If all tests hold, we append the token and its label to the current group\n",
    "                group_tmp.append((token, label))\n",
    "        elif label == 'O':\n",
    "            # An 'O' Tag is always outside. Thus if we scan one, it means that the current group is over\n",
    "            if group_tmp:\n",
    "                groups.append((entity, group_tmp))\n",
    "                group_tmp = [] # New Group will begin\n",
    "            group_started = False\n",
    "        else:\n",
    "            pass\n",
    "            # print(f\"Filtered faulty classification: ({token}, {label})\") @todo handle this as error log\n",
    "\n",
    "    # Flush last word\n",
    "    if group_tmp:\n",
    "        groups.append((entity, group_tmp))\n",
    "\n",
    "    return groups\n",
    "\n",
    "def insert_paragraph(speech_id:int, index:int, entity:str, group_clean_text:str, paragraph:str):\n",
    "    \"\"\"\n",
    "    Inserts a classified paragraph into the database.\n",
    "\n",
    "    Args:\n",
    "        speech_id (int): The ID of the speech.\n",
    "        index (int): The index of the paragraph in the speech. 0 for the first paragraph, 1 for the second, etc.\n",
    "        entity (str): The entity label of the paragraph. For example, 'EOPOL' for B-EOPOL.\n",
    "        group_clean_text (str): The cleaned text of the paragraph.\n",
    "        paragraph (str): The original paragraph text.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    con.execute(\"\"\"\n",
    "        INSERT INTO group_mention (paragraph_no, speech_id, paragraph, group_text, label)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "        ON CONFLICT DO NOTHING; -- If the paragraph already exists, do nothing\n",
    "    \"\"\", (index, speech_id, paragraph, group_clean_text, entity))\n",
    "    con.commit()\n",
    "\n",
    "def insert_group_mention(speech_id:str, index:int, groups:list[tuple[str,list[tuple[str,str]]]], paragraph:str):\n",
    "    \"\"\"Inserts classified paragraphs into the database.\n",
    "\n",
    "    Args:\n",
    "        speech_id (str): The ID of the speech.\n",
    "        index (int): The index of the paragraph in the speech. 0 for the first paragraph, 1 for the second, etc.\n",
    "        groups (list[tuple[str,list[tuple[str,str]]]]): A list of tuples containing the entity and a list of the token, label pairs for each group.\n",
    "        paragraph (str): The original paragraph text.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for group in groups:\n",
    "        entity, raw_tokens = group\n",
    "        tokens = [item[0] for item in raw_tokens]\n",
    "        group_clean_text = smart_join(tokens)\n",
    "        # print(f\"{entity} -> {group_clean_text}\")\n",
    "        insert_paragraph(speech_id, index, entity, group_clean_text, paragraph)\n",
    "\n",
    "\n",
    "def process_speech(speech_id:str, paragraphs:list[dict[str, list[str]]]):\n",
    "    \"\"\"Processes a speech by classifying its paragraphs (extracting group mention) and inserting them into the database.\n",
    "\n",
    "    Args:\n",
    "        speech_id (str): The ID of the speech.\n",
    "        paragraphs (list[dict[str, list[str]]]): The list of paragraphs, each represented as a dictionary with a 'paragraphs' key containing the text.\n",
    "    \"\"\"\n",
    "    group_mention = predict_batch(paragraphs)\n",
    "    for index, p in enumerate(group_mention):\n",
    "        # print(p)\n",
    "        groups = extract_groups(p)\n",
    "        insert_group_mention(speech_id, index, groups, paragraphs[index].get('paragraphs'))\n",
    "\n",
    "def extract_speech() -> tuple:\n",
    "    #@todo speed up this query -> extract more than 1 entry at a time :)\n",
    "    \"\"\"Extracts a random speech from the database that has not been processed yet.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the speech data, including its ID, title, date, and text content.\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "        SELECT *\n",
    "        FROM speech\n",
    "        WHERE position NOT IN ('Präsidentin', 'Vizepräsidentin', 'Vizepräsident', 'Präsident')\n",
    "              AND id NOT IN (SELECT speech_id FROM group_mention) -- check that speech wasn't already processed\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "    return con.execute(sql).fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4735c821-6c3d-451d-9c2f-df43b7aa43c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::   0%|          | 10/2000 [00:07<25:07,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  11%|█▏        | 227/2000 [02:41<20:46,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  16%|█▌        | 320/2000 [03:45<18:07,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  18%|█▊        | 358/2000 [04:13<18:41,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  18%|█▊        | 366/2000 [04:19<20:32,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n",
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  22%|██▏       | 433/2000 [05:07<17:20,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  26%|██▌       | 520/2000 [06:06<16:45,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  34%|███▍      | 689/2000 [08:06<14:54,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  38%|███▊      | 762/2000 [08:55<13:04,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  39%|███▊      | 772/2000 [09:02<13:47,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  39%|███▉      | 776/2000 [09:05<13:55,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  41%|████      | 812/2000 [09:30<13:47,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  43%|████▎     | 853/2000 [09:58<12:31,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  43%|████▎     | 861/2000 [10:04<12:49,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  46%|████▋     | 926/2000 [10:49<11:52,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unexpected tag: kommentar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  49%|████▉     | 987/2000 [11:31<11:48,  1.43it/s]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\" Main function to process speeches and extract group mentions.\n",
    "    \"\"\"\n",
    "    create_paragraphs_classified_table(reset_db=False)\n",
    "    for i in tqdm(range(2000), desc='Processing:'):\n",
    "        speech = extract_speech()\n",
    "        speech_id, paragraphs = preprocess_speech(speech)\n",
    "        process_speech(speech_id, paragraphs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdcc0b7-67bf-4198-ac5c-b00050d4d19d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
