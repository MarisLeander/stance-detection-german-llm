{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20bc4d60-4174-4a55-8886-f9cea1d8a8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting imports\n",
      "Import successful\n"
     ]
    }
   ],
   "source": [
    "# Since the imports take quite some time we Signal if they are done\n",
    "print(\"Starting imports\")\n",
    "from collections import Counter\n",
    "import duckdb\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import time\n",
    "import multiprocessing \n",
    "from tqdm import tqdm\n",
    "# Import classifier\n",
    "from group_classifier import GroupClassifier\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "# Disables progress bar of .map function\n",
    "disable_progress_bar()\n",
    "print(\"Import successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e9c2acf-64d4-4cf4-a816-c9315fef11f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to sql database\n",
    "con = duckdb.connect(database='../data/database/german-parliament.duckdb', read_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87a49673-3f68-4987-a75c-4ec16e6ed945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_speech(speech: tuple) -> tuple[int, list[dict[str, str]], Counter]:\n",
    "    \"\"\"The filtering for skipping_president_remarks is only necessare for periods >= 19 because of the \"new\" format provided by the bundestag. \n",
    "    For periods <19 the method just extracs all the 'p' tags\n",
    "\n",
    "    Args:\n",
    "        speech (tuple): A tuple containing the speech data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the speech ID and a list of paragraphs with their text content.\n",
    "    \"\"\"\n",
    "    speech_id = speech.id\n",
    "    text_content = speech.content\n",
    "\n",
    "    # Parse xml from string will throw ParseError if not parseable\n",
    "    root = ET.fromstring(text_content)\n",
    "    num_of_important_paragraphs = 0\n",
    "    num_of_redner_information = 0\n",
    "    num_of_comments = 0\n",
    "    num_of_president_paragraphs = 0\n",
    "    num_of_mislabeled_paragraphs = 0\n",
    "    \n",
    "    paragraphs_text = []\n",
    "    # skipping_president_remarks is used to detect interruptions of the President\n",
    "    skipping_president_remarks = False\n",
    "\n",
    "    # Iterate over all direct children of the root <rede> element\n",
    "    for element in root:\n",
    "        # 1. Check if we need to STOP skipping\n",
    "        if skipping_president_remarks:\n",
    "            if element.tag == 'p' and element.attrib.get(\"klasse\") == \"redner\":\n",
    "                num_of_redner_information += 1\n",
    "                # We know that the interruption of the president ended and the speech continues after this tag\n",
    "                skipping_president_remarks = False\n",
    "                continue \n",
    "            elif element.tag == 'p':\n",
    "                num_of_president_paragraphs += 1\n",
    "                # Do nothing -> president is speaking\n",
    "                continue\n",
    "            elif element.tag == 'kommentar':\n",
    "                num_of_comments += 1\n",
    "                # Do nothing -> comment from the crowd, which we are not interested in\n",
    "                continue\n",
    "            else:\n",
    "                num_of_mislabeled_paragraphs += 1\n",
    "                print(f\"Detected unexpected tag: {element.tag}\")\n",
    "\n",
    "        # 2. Check if we need to START skipping (President speaks)\n",
    "        if element.tag == 'name':\n",
    "            num_of_redner_information += 1\n",
    "            # Check if the text content of the name tag indicates a presiding\n",
    "            name_text = \"\".join(element.itertext()).strip() # Gets all text within <name>, including children\n",
    "            # State all titles which will get ignored\n",
    "            president_titles = [\"Präsidentin\", \"Präsident\", \"Vizepräsidentin\", \"Vizepräsident\"]\n",
    "            if name_text and any(title in name_text for title in president_titles):\n",
    "                skipping_president_remarks = True\n",
    "                continue # Move to the next element, don't process this <name> tag as a paragraph\n",
    "\n",
    "        # 3. Process <p> tags\n",
    "        if element.tag == 'p':\n",
    "            # Filter out speaker information paragraphs\n",
    "            if element.attrib.get(\"klasse\") == \"redner\":\n",
    "                num_of_redner_information += 1\n",
    "                continue\n",
    "            else:\n",
    "                num_of_important_paragraphs += 1\n",
    "                # Actual text content of speech!\n",
    "                # Get text of the paragraph and remove potential irrelevant whitespaces\n",
    "                p_text = element.text.strip() if element.text else \"\"\n",
    "                \n",
    "                item = {\"paragraphs\": p_text}\n",
    "                paragraphs_text.append(item)\n",
    "        # 4. Process any other tags\n",
    "        elif element.tag == 'kommentar' or  element.tag == 'stage':\n",
    "            # Comments from the crowd are labeled as 'stage' in periods < 19\n",
    "            num_of_comments += 1\n",
    "        elif element.tag == 'speaker':\n",
    "            num_of_redner_information += 1\n",
    "        else:\n",
    "            num_of_mislabeled_paragraphs += 1\n",
    "            print(f\"Detected unexpected tag: {element.tag}\")\n",
    "        \n",
    "        \n",
    "    # Create the counter\n",
    "    stats_dict = Counter({\n",
    "        \"num_of_important_paragraphs\": num_of_important_paragraphs,\n",
    "        \"num_of_redner_information\": num_of_redner_information,\n",
    "        \"num_of_comments\": num_of_comments,\n",
    "        \"num_of_president_paragraphs\": num_of_president_paragraphs,\n",
    "        \"num_of_mislabeled_paragraphs\": num_of_mislabeled_paragraphs,\n",
    "    })\n",
    "\n",
    "    return (speech_id, paragraphs_text, stats_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be7eee91013bc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paragraphs_classified_table(reset_db:bool=False):\n",
    "    \"\"\"\n",
    "    Creates a table for classified paragraphs in the database.\n",
    "\n",
    "    Args:\n",
    "        reset_db (bool): If True, drops the table if it exists before creating it.\n",
    "                         Defaults to False.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if reset_db:\n",
    "        con.execute(\"DROP TABLE IF EXISTS group_mention\")\n",
    "        con.execute(\"DROP SEQUENCE IF EXISTS group_mention_id_seq\")\n",
    "\n",
    "    # Create a sequence for the primary key\n",
    "    con.execute(\"CREATE SEQUENCE IF NOT EXISTS group_mention_id_seq START 1;\")\n",
    "\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS group_mention (\n",
    "            id INTEGER DEFAULT nextval('group_mention_id_seq') PRIMARY KEY,\n",
    "            paragraph_no INTEGER, -- If its 0, its the first paragraph of the speech, 1 for the second, etc.\n",
    "            speech_id VARCHAR NOT NULL REFERENCES speech(id),\n",
    "            paragraph VARCHAR NOT NULL,\n",
    "            group_text VARCHAR NOT NULL, -- This is the group mention, e.g. die Mitglieder der SPD-Fraktion\n",
    "            label VARCHAR(15) NOT NULL,\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f07d0a096dc1c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_join(tokens):\n",
    "    \"\"\"\n",
    "    Joins a list of word tokens into a single string, handling punctuation\n",
    "    and sub-word prefixes ('##') correctly.\n",
    "\n",
    "    Args:\n",
    "        tokens (list[str]): A list of word tokens, which may include sub-word tokens prefixed with '##'.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string with tokens joined together, ensuring proper spacing around punctuation.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # Define punctuation that should not have a preceding space\n",
    "    no_space_before = {',', '.', '?', '!', ';', ':', ')'}\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        # If it's the very first token, a punctuation mark, or a sub-word,\n",
    "        # don't add a leading space.\n",
    "        if i > 0 and token not in no_space_before and not token.startswith('##'):\n",
    "            result.append(' ')\n",
    "\n",
    "        # Append the token itself, removing any '##' prefixes\n",
    "        result.append(token.replace('##', ''))\n",
    "\n",
    "    return \"\".join(result).replace(' - ', '-') # Removes the space before and after a hyphen (Bindestrich)\n",
    "\n",
    "def extract_groups(paragraph:list[tuple[str,str]]) -> list[tuple[str, str]]:\n",
    "    \"\"\" Extracts group mention along with their labels from a paragraph. It groups tokens by their entity labels to get the full mention.\n",
    "    If a mention is broken e.g it does not start with a 'B-' label, it will be filtered.\n",
    "\n",
    "    Args:\n",
    "        paragraph (list[tuple[str,str]]): A list of tuples containing tokens and their corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[str, str]]: A list of tuples where each tuple contains the entity label and a list of (token, label) pairs for that entity, which contain the full mention.\n",
    "    \"\"\"\n",
    "    # This is a set of special tokens that should be ignored in the grouping process. -> adjust it if necessary\n",
    "    SPECIAL = {\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[UNK]\"}\n",
    "    # empty list for groups of paragraph\n",
    "    groups = []\n",
    "    # This is a temporary list to hold the current group mention\n",
    "    group_tmp = []\n",
    "    # This is a flag to indicate if we are currently inside a group mention\n",
    "    group_started = False\n",
    "    entity = \"\" # This hold the current entity. e.g. EOPOL for B-EOPOL\n",
    "    for token, label in paragraph:\n",
    "        # If token is a special token like [CLS] skip it\n",
    "        if token in SPECIAL:\n",
    "            continue\n",
    "        # Check for begin of group\n",
    "        elif label.startswith(\"B-\"):\n",
    "            group_started = True\n",
    "            entity = label[2:]\n",
    "            if group_tmp:\n",
    "                groups.append((entity, group_tmp))\n",
    "                group_tmp = [] # New Group will begin\n",
    "            # Append new beginning label\n",
    "            group_tmp.append((token, label))\n",
    "        # It is checked that 1) we have an inside label and 2) There was a B- label before!\n",
    "        elif label.startswith(\"I-\") and group_started:\n",
    "            # Then we check if the entity matches\n",
    "            if label[2:] != entity:\n",
    "                # print(f\"Current label: {label[2:]} doesn't match beginning label: {entity}\") @todo handle this as error log\n",
    "                # Break current group because of the miss-label\n",
    "                group_started = False\n",
    "            else:\n",
    "            # If all tests hold, we append the token and its label to the current group\n",
    "                group_tmp.append((token, label))\n",
    "        elif label == 'O':\n",
    "            # An 'O' Tag is always outside. Thus if we scan one, it means that the current group is over\n",
    "            if group_tmp:\n",
    "                groups.append((entity, group_tmp))\n",
    "                group_tmp = [] # New Group will begin\n",
    "            group_started = False\n",
    "        else:\n",
    "            pass\n",
    "            # print(f\"Filtered faulty classification: ({token}, {label})\") @todo handle this as error log\n",
    "\n",
    "    # Flush last word\n",
    "    if group_tmp:\n",
    "        groups.append((entity, group_tmp))\n",
    "\n",
    "    return groups\n",
    "\n",
    "def insert_paragraph(speech_id:int, index:int, entity:str, group_clean_text:str, paragraph:str):\n",
    "    \"\"\"\n",
    "    Inserts a classified paragraph into the database.\n",
    "\n",
    "    Args:\n",
    "        speech_id (int): The ID of the speech.\n",
    "        index (int): The index of the paragraph in the speech. 0 for the first paragraph, 1 for the second, etc.\n",
    "        entity (str): The entity label of the paragraph. For example, 'EOPOL' for B-EOPOL.\n",
    "        group_clean_text (str): The cleaned text of the paragraph.\n",
    "        paragraph (str): The original paragraph text.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    con.execute(\"\"\"\n",
    "        INSERT INTO group_mention (paragraph_no, speech_id, paragraph, group_text, label)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "        ON CONFLICT DO NOTHING; -- If the paragraph already exists, do nothing\n",
    "    \"\"\", (index, speech_id, paragraph, group_clean_text, entity))\n",
    "    con.commit()\n",
    "\n",
    "def insert_group_mention(speech_id:str, index:int, groups:list[tuple[str,list[tuple[str,str]]]], paragraph:str):\n",
    "    \"\"\"Inserts classified paragraphs into the database.\n",
    "\n",
    "    Args:\n",
    "        speech_id (str): The ID of the speech.\n",
    "        index (int): The index of the paragraph in the speech. 0 for the first paragraph, 1 for the second, etc.\n",
    "        groups (list[tuple[str,list[tuple[str,str]]]]): A list of tuples containing the entity and a list of the token, label pairs for each group.\n",
    "        paragraph (str): The original paragraph text.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for group in groups:\n",
    "        entity, raw_tokens = group\n",
    "        tokens = [item[0] for item in raw_tokens]\n",
    "        group_clean_text = smart_join(tokens)\n",
    "        # print(f\"{entity} -> {group_clean_text}\")\n",
    "        insert_paragraph(speech_id, index, entity, group_clean_text, paragraph)\n",
    "\n",
    "\n",
    "def process_speech(speech_id:str, paragraphs:list[dict[str, list[str]]]):\n",
    "    \"\"\"Processes a speech by classifying its paragraphs (extracting group mention) and inserting them into the database.\n",
    "\n",
    "    Args:\n",
    "        speech_id (str): The ID of the speech.\n",
    "        paragraphs (list[dict[str, list[str]]]): The list of paragraphs, each represented as a dictionary with a 'paragraphs' key containing the text.\n",
    "    \"\"\"\n",
    "    group_mention = predict_batch(paragraphs)\n",
    "    for index, p in enumerate(group_mention):\n",
    "        # print(p)\n",
    "        groups = extract_groups(p)\n",
    "        insert_group_mention(speech_id, index, groups, paragraphs[index].get('paragraphs'))\n",
    "\n",
    "def extract_speeches() -> pd.DataFrame:\n",
    "    #@todo speed up this query -> extract more than 1 entry at a time :)\n",
    "    \"\"\"Extracts a random speech from the database that has not been processed yet.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the speech data, including its ID, title, date, and text content.\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "        SELECT *\n",
    "        FROM speech\n",
    "        WHERE position NOT IN ('Präsidentin', 'Vizepräsidentin', 'Vizepräsident', 'Präsident')\n",
    "              OR position IS NULL\n",
    "              AND id NOT IN (SELECT speech_id FROM group_mention) -- check that speech wasn't already processed\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT 10_000\n",
    "        \"\"\"\n",
    "    return con.execute(sql).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4735c821-6c3d-451d-9c2f-df43b7aa43c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiprocessing start method set to 'spawn'.\n",
      "--- Starting Batch Speech Processing ---\n",
      "Loading model from ../models/bert-base-german-cased-finetuned-MOPE-L3_Run_3_Epochs_29 to cuda...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find BertForTokenClassification neither in <module 'transformers.models.bert' from '/home/ma/ma_ma/ma_mbuttman/.local/lib/python3.11/site-packages/transformers/models/bert/__init__.py'> nor in <module 'transformers' from '/home/ma/ma_ma/ma_mbuttman/.local/lib/python3.11/site-packages/transformers/__init__.py'>!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:708\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:712\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Could not find BertForTokenClassification in <module 'transformers' from '/home/ma/ma_ma/ma_mbuttman/.local/lib/python3.11/site-packages/transformers/__init__.py'>!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# This can happen if the context is already set, which is fine.\u001b[39;00m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# con.close()\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load ingthe classifier model once at the start.\u001b[39;00m\n\u001b[32m      8\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33m../models/bert-base-german-cased-finetuned-MOPE-L3_Run_3_Epochs_29\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m classifier = \u001b[43mGroupClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Prepare the database table.\u001b[39;00m\n\u001b[32m     12\u001b[39m create_paragraphs_classified_table(reset_db=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pfs/data6/home/ma/ma_ma/ma_mbuttman/stance-detection-german-llm/extract-group-mention/group_classifier.py:53\u001b[39m, in \u001b[36mGroupClassifier.__init__\u001b[39m\u001b[34m(self, model_dir, device)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = AutoTokenizer.from_pretrained(\u001b[38;5;28mself\u001b[39m.model_dir, use_fast=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Using float16 for A100 performance\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Optional: Compile the model for a potential speed boost on PyTorch 2.0+\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# The first prediction will be slower due to a one-time compilation cost.\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# self.model = torch.compile(self.model, mode=\"max-autotune\")\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:568\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    565\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    566\u001b[39m     )\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m     model_class = \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:388\u001b[39m, in \u001b[36m_get_model_class\u001b[39m\u001b[34m(config, model_mapping)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     supported_models = \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    390\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:774\u001b[39m, in \u001b[36m_LazyAutoMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_mapping:\n\u001b[32m    773\u001b[39m     model_name = \u001b[38;5;28mself\u001b[39m._model_mapping[model_type]\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[32m    777\u001b[39m model_types = [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._config_mapping.items() \u001b[38;5;28;01mif\u001b[39;00m v == key.\u001b[34m__name__\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:788\u001b[39m, in \u001b[36m_LazyAutoMapping._load_attr_from_module\u001b[39m\u001b[34m(self, model_type, attr)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m    787\u001b[39m     \u001b[38;5;28mself\u001b[39m._modules[module_name] = importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtransformers.models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:710\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    708\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m getattribute_from_module(transformers_module, attr)\n\u001b[32m    709\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m neither in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m nor in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    712\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Could not find BertForTokenClassification neither in <module 'transformers.models.bert' from '/home/ma/ma_ma/ma_mbuttman/.local/lib/python3.11/site-packages/transformers/models/bert/__init__.py'> nor in <module 'transformers' from '/home/ma/ma_ma/ma_mbuttman/.local/lib/python3.11/site-packages/transformers/__init__.py'>!"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to efficiently process all speeches in a batch.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Batch Speech Processing ---\")\n",
    "    \n",
    "    # Load ingthe classifier model once at the start.\n",
    "    model_path = \"../models/bert-base-german-cased-finetuned-MOPE-L3_Run_3_Epochs_29\"\n",
    "    classifier = GroupClassifier(model_dir=model_path)\n",
    "    \n",
    "    # Prepare the database table.\n",
    "    create_paragraphs_classified_table(reset_db=True)\n",
    "    \n",
    "    # Fetch all speeches from the database.\n",
    "    speeches_df = extract_speeches()\n",
    "    \n",
    "    all_paragraphs_text = []\n",
    "    # This list will store metadata to remember where each paragraph came from.\n",
    "    # Each item will be a tuple: (speech_id, original_paragraph_index)\n",
    "    paragraph_metadata = [] \n",
    "    \n",
    "    overall_statistics = Counter({\n",
    "        \"num_of_important_paragraphs\": 0,\n",
    "        \"num_of_redner_information\": 0,\n",
    "        \"num_of_comments\": 0,\n",
    "        \"num_of_president_paragraphs\": 0,\n",
    "        \"num_of_mislabeled_paragraphs\": 0,\n",
    "    })\n",
    "    \n",
    "    print(\"\\nPreprocessing speeches and collecting all paragraphs...\")\n",
    "    for _, row in tqdm(speeches_df.iterrows(), total=len(speeches_df), desc=\"Gathering Paragraphs\"):\n",
    "        speech_id, paragraphs_list_of_dicts, item_statistics = preprocess_speech(row)\n",
    "        # Add counter to our stats\n",
    "        overall_statistics += item_statistics\n",
    "        for i, para_dict in enumerate(paragraphs_list_of_dicts):\n",
    "            # Assuming the text is in para_dict['paragraphs']\n",
    "            paragraph_text = para_dict.get('paragraphs')\n",
    "            if paragraph_text:\n",
    "                all_paragraphs_text.append(paragraph_text)\n",
    "                paragraph_metadata.append((speech_id, i)) # Save the origin\n",
    "    \n",
    "    if not all_paragraphs_text:\n",
    "        print(\"No paragraphs to process. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. EFFICIENT BATCHED INFERENCE ---\n",
    "    # This is the magic step. Process all paragraphs in one go on the GPU.\n",
    "    # Use a large batch size to maximize A100 utilization.\n",
    "    print(f\"\\nStarting batch prediction on {len(all_paragraphs_text)} paragraphs...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    all_predictions = classifier.predict(all_paragraphs_text, batch_size=256)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"--- Prediction finished in {end_time - start_time:.2f} seconds ---\")\n",
    "\n",
    "    # --- 4. DATABASE INSERTION ---\n",
    "    # Now, we loop through the results and metadata, which are in the same order.\n",
    "    print(\"\\nExtracting groups and inserting results into the database...\")\n",
    "    for i, metadata in enumerate(tqdm(paragraph_metadata, desc=\"Inserting Records\")):\n",
    "        speech_id, paragraph_index = metadata\n",
    "        \n",
    "        # Get the corresponding prediction and original text\n",
    "        predicted_tokens_and_labels = all_predictions[i]\n",
    "        original_paragraph_text = all_paragraphs_text[i]\n",
    "        \n",
    "        # Use your existing functions to process the results\n",
    "        groups = extract_groups(predicted_tokens_and_labels)\n",
    "        insert_group_mention(speech_id, paragraph_index, groups, original_paragraph_text)\n",
    "        \n",
    "    print(\"\\n--- Processing complete! ---\")\n",
    "    \n",
    "    # 3. The final result is a Counter object (which works just like a dict)\n",
    "    print(f\"\\nAfter loop: {overall_statistics}\")\n",
    "    print(f\"Overall extracted paragraphs: {overall_statistics.total()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b179ce-6094-421e-a1d0-47b425d2e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df100e4-b32c-4cfd-b360-d238d089237a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM Project - VENV)",
   "language": "python",
   "name": "my-llm-project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
