{
 "cells": [
  {
   "cell_type": "code",
   "id": "a0ded3fa9169d5e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T11:18:37.025959Z",
     "start_time": "2025-06-04T11:18:32.809540Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "print(\"Import successful\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marisbuttmann/PycharmProjects/stance-detection-german-llm/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "192c407f9481815a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T11:35:16.715608Z",
     "start_time": "2025-06-04T11:35:16.703517Z"
    }
   },
   "source": [
    "def index2label(index):\n",
    "    \"\"\" Convert an index to a label.\n",
    "\n",
    "    Args:\n",
    "        index (int): The index to be converted.\n",
    "\n",
    "    Returns:\n",
    "        str: The label corresponding\n",
    "    \"\"\"\n",
    "\n",
    "    labels = {0: '[PAD]', 1: '[UNK]', 2: 'B-EGPOL', 3: 'B-EOFINANZ', 4: 'B-EOMEDIA', 5: 'B-EOMIL', 6: 'B-EOMOV', 7: 'B-EONGO', 8: 'B-EOPOL', 9: 'B-EOREL', 10: 'B-EOSCI', 11: 'B-EOWIRT', 12: 'B-EPFINANZ', 13: 'B-EPKULT', 14: 'B-EPMEDIA', 15: 'B-EPMIL', 16: 'B-EPMOV', 17: 'B-EPNGO', 18: 'B-EPPOL', 19: 'B-EPREL', 20: 'B-EPSCI', 21: 'B-EPWIRT', 22: 'B-GPE', 23: 'B-PAGE', 24: 'B-PETH', 25: 'B-PFUNK', 26: 'B-PGEN', 27: 'B-PNAT', 28: 'B-PSOZ', 29: 'I-EGPOL', 30: 'I-EOFINANZ', 31: 'I-EOMEDIA', 32: 'I-EOMIL', 33: 'I-EOMOV', 34: 'I-EONGO', 35: 'I-EOPOL', 36: 'I-EOREL', 37: 'I-EOSCI', 38: 'I-EOWIRT', 39: 'I-EPFINANZ', 40: 'I-EPKULT', 41: 'I-EPMEDIA', 42: 'I-EPMIL', 43: 'I-EPMOV', 44: 'I-EPNGO', 45: 'I-EPPOL', 46: 'I-EPREL', 47: 'I-EPSCI', 48: 'I-EPWIRT', 49: 'I-GPE', 50: 'I-PAGE', 51: 'I-PETH', 52: 'I-PFUNK', 53: 'I-PGEN', 54: 'I-PNAT', 55: 'I-PSOZ', 56: 'O'}\n",
    "\n",
    "    return labels[index]\n",
    "    #\n",
    "    # labels = [\"[PAD]\", \"[UNK]\", \"B-EGPOL\", \"B-EOFINANZ\", \"B-EOMEDIA\", \"B-EOMIL\", \"B-EOMOV\", \"B-EONGO\", \"B-EOPOL\", \"B-EOREL\", \"B-EOSCI\", \"B-EOWIRT\", \"B-EPFINANZ\", \"B-EPKULT\", \"B-EPMEDIA\", \"B-EPMIL\", \"B-EPMOV\", \"B-EPNGO\", \"B-EPPOL\", \"B-EPREL\", \"B-EPSCI\", \"B-EPWIRT\", \"B-GPE\", \"B-PAGE\", \"B-PETH\", \"B-PFUNK\", \"B-PGEN\", \"B-PNAT\", \"B-PSOZ\", \"I-EGPOL\", \"I-EOFINANZ\", \"I-EOMEDIA\", \"I-EOMIL\", \"I-EOMOV\", \"I-EONGO\", \"I-EOPOL\", \"I-EOREL\", \"I-EOSCI\", \"I-EOWIRT\", \"I-EPFINANZ\", \"I-EPKULT\", \"I-EPMEDIA\", \"I-EPMIL\", \"I-EPMOV\", \"I-EPNGO\", \"I-EPPOL\", \"I-EPREL\", \"I-EPSCI\", \"I-EPWIRT\", \"I-GPE\", \"I-PAGE\", \"I-PETH\", \"I-PFUNK\", \"I-PGEN\", \"I-PNAT\", \"I-PSOZ\", \"O\"]\n",
    "    # label2index, index2label = {}, {}\n",
    "    # for i, item in enumerate(labels):\n",
    "    #     label2index[item] = i\n",
    "    #     index2label[i] = item\n",
    "\n",
    "def load_model(model_dir):\n",
    "    \"\"\" Load a pre-trained model from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str or Path): The directory where the model is stored.\n",
    "\n",
    "    Returns:\n",
    "        model (AutoModelForTokenClassification): The loaded model.\n",
    "    \"\"\"\n",
    "    # Load the config\n",
    "    cfg   = AutoConfig.from_pretrained(model_dir)\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_dir, config=cfg).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return model\n",
    "\n",
    "def tokenize_labels(data, tokenizer):\n",
    "    \"\"\" Tokenize the input data. This is needed to prepare the input data for the model.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The input data containing the paragraphs to be tokenized.\n",
    "        tokenizer (AutoTokenizer): The tokenizer to be used for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        tokenized_inputs (dict): A dictionary containing the tokenized inputs.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(data[\"paragraphs\"],\n",
    "                                  truncation=True,\n",
    "                                  padding=True,\n",
    "                                  is_split_into_words=False)\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "def encode_dataset(raw_data: List[Dict[str, List[str]]], tokenizer):\n",
    "    # data = load_dataset('json', data_files={'input_data': path}) -> Use this to import from json\n",
    "    ds = Dataset.from_list(raw_data)\n",
    "    data = DatasetDict({\"input_data\": ds})\n",
    "    encoded_data = data.map(\n",
    "                tokenize_labels,              # ← function reference\n",
    "                batched=True,\n",
    "                fn_kwargs={\"tokenizer\": tokenizer},  # extra objects you need\n",
    "                remove_columns=[\"paragraphs\"]# only keep the columns that are needed, i.e. input_ids, attention_mask, token_type_ids and labels. It checks if the columns are present in the corpus and removes them if they are not needed.\n",
    "            )\n",
    "    dataset = encoded_data[\"input_data\"].with_format(\"torch\")  # Convert to PyTorch format, to be compatible with DataLoader\n",
    "    return dataset\n",
    "\n",
    "def build_dataloader(dataset, tokenizer, batch_size=16):\n",
    "    # Create a DataLoader for the test dataset. The DataCollatorWithPadding will pad the sequences to the one with the maximum length in the batch, so that all sequences in the batch have the same length.\n",
    "    # The dataloader will return batches of data, which we can then pass to the model for prediction.\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "def merge_word_pieces(paired_tokens):\n",
    "    \"\"\"\n",
    "    paired_tokens : List[Tuple[str, str]]\n",
    "        e.g. [(\"Beispiel\",\"O\"), (\"##satz\",\"O\"), ...]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    merged : List[Tuple[str, str]]\n",
    "        Word-level (token, label) pairs.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    current_word = \"\"\n",
    "    current_label = None\n",
    "\n",
    "    # Maybe adjust this set to tokenizer's special tokens\n",
    "    SPECIAL = {\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[UNK]\"}\n",
    "\n",
    "    for token, label in paired_tokens:\n",
    "        # 1. drop specials outright\n",
    "        if token in SPECIAL:\n",
    "            continue\n",
    "\n",
    "        # 2. continuation piece?\n",
    "        if token.startswith(\"##\"):\n",
    "            current_word += token[2:]        # append stem\n",
    "            continue                         # label already set\n",
    "        else:\n",
    "            # 3. flush previous buffered word\n",
    "            if current_word:\n",
    "                merged.append((current_word, current_label))\n",
    "            # 4. start new word buffer\n",
    "            current_word  = token\n",
    "            current_label = label\n",
    "\n",
    "    # flush last word\n",
    "    if current_word:\n",
    "        merged.append((current_word, current_label))\n",
    "    return merged\n",
    "\n",
    "def get_predictions(dataloader, model, tokenizer):\n",
    "\n",
    "    preds_all= [] # Initialize lists to store predictions\n",
    "\n",
    "    for batch in dataloader:\n",
    "        print(\"Batch size:\", len(batch[\"input_ids\"]))\n",
    "        with torch.inference_mode():\n",
    "            output = model(**batch) # unpacks the batch dictionary and passes the input IDs and attention mask to the model, which will return the logits, which are the unnormalized scores for each label.\n",
    "\n",
    "        logits = output.logits                    # Logits are the unnormalized scores for each label.\n",
    "        preds  = torch.argmax(logits, dim=-1)     # take the best label for each token. We use argmax for inference since we don't need to compute the loss during inference, we just want the predicted labels. Also we won't do majority voting since (now) we only use one model.\n",
    "        for i in range(len(preds)):\n",
    "            labels = [index2label(int(i)) for i in preds[i]]\n",
    "            tokens = tokenizer.convert_ids_to_tokens(batch[\"input_ids\"][i])\n",
    "            # preds_all.append(merge_word_pieces(zip(tokens,labels)))\n",
    "            preds_all.append(list(zip(tokens, labels)))  # Extend the list with the new predictions\n",
    "    return preds_all\n",
    "    #@ todo maybe store the predictions in a list and compare them with the ground truth labels!!!\n",
    "\n",
    "def predict_batch(data: List[Dict[str, List[str]]]) -> List[List[Tuple[str, str]]]: #@todo add return\n",
    "    model_dir = Path(\"../models/bert-base-german-cased-finetuned-MOPE-L3_Run_2_Epochs_29\")\n",
    "    tokenizer   = AutoTokenizer.from_pretrained(model_dir, use_fast=True) # use_fast=True enables the fast tokenizer implementation\n",
    "    model = load_model(model_dir)\n",
    "    encoded_dataset = encode_dataset(data, tokenizer)\n",
    "    dataloader = build_dataloader(encoded_dataset, tokenizer, batch_size=3)\n",
    "    return get_predictions(dataloader, model, tokenizer)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "60c3c3003d0255be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T11:35:17.218582Z",
     "start_time": "2025-06-04T11:35:16.990397Z"
    }
   },
   "source": [
    "# paragraphs = [{'paragraphs': 'Meine Damen und Herren, die Beantwortung der Interpellation ist erfolgt. Ich frage, ob eine Besprechung der Interpellation gewünscht wird. — Das ist nicht der Fall. Damit ist Punkt 1 der Tagesordnung erledigt. Ich rufe auf Punkt 2 der Tagesordnung:'}, {'paragraphs': 'Erste Beratung des Entwurfs eines Gesetzes zur Regelung der Besteuerung des Kleinpflanzertabaks im Erntejahr 1950 (Nr. 1508 der Drucksachen).'}, {'paragraphs': 'Dazu hat zunächst das Wort Herr Staatssekretär Hartmann.'}]\n",
    "#\n",
    "# classified_paragraphs = predict_batch(paragraphs)\n",
    "# print(classified_paragraphs)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1474.96 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T11:35:18.023921Z",
     "start_time": "2025-06-04T11:35:18.020781Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2fe4220ad737d34c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('[CLS]', 'O'), ('Meine', 'O'), ('Damen', 'O'), ('und', 'O'), ('Herren', 'O'), (',', 'O'), ('die', 'O'), ('Beantwortung', 'O'), ('der', 'O'), ('Inter', 'O'), ('##pel', 'O'), ('##lat', 'O'), ('##ion', 'O'), ('ist', 'O'), ('erfolgt', 'O'), ('.', 'O'), ('Ich', 'O'), ('frag', 'O'), ('##e', 'O'), (',', 'O'), ('ob', 'O'), ('eine', 'O'), ('Besprech', 'O'), ('##ung', 'O'), ('der', 'O'), ('Inter', 'O'), ('##pel', 'O'), ('##lat', 'O'), ('##ion', 'O'), ('gewünscht', 'O'), ('wird', 'O'), ('.', 'O'), ('[UNK]', 'O'), ('Das', 'O'), ('ist', 'O'), ('nicht', 'O'), ('der', 'O'), ('Fall', 'O'), ('.', 'O'), ('Damit', 'O'), ('ist', 'O'), ('Punkt', 'O'), ('1', 'O'), ('der', 'O'), ('Tages', 'O'), ('##ordnung', 'O'), ('erledigt', 'O'), ('.', 'O'), ('Ich', 'O'), ('ru', 'O'), ('##fe', 'O'), ('auf', 'O'), ('Punkt', 'O'), ('2', 'O'), ('der', 'O'), ('Tages', 'O'), ('##ordnung', 'O'), (':', 'O'), ('[SEP]', 'O')], [('[CLS]', 'O'), ('Erste', 'O'), ('Beratung', 'O'), ('des', 'O'), ('Entwurf', 'O'), ('##s', 'O'), ('eines', 'O'), ('Gesetzes', 'O'), ('zur', 'O'), ('Regelung', 'O'), ('der', 'O'), ('Besteuerung', 'O'), ('des', 'O'), ('Klein', 'O'), ('##pflanz', 'O'), ('##ert', 'O'), ('##aba', 'O'), ('##ks', 'O'), ('im', 'O'), ('Ernte', 'O'), ('##jahr', 'O'), ('1950', 'O'), ('(', 'O'), ('Nr', 'O'), ('.', 'O'), ('150', 'O'), ('##8', 'O'), ('der', 'O'), ('Druck', 'O'), ('##sachen', 'O'), (')', 'O'), ('.', 'O'), ('[SEP]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O')], [('[CLS]', 'O'), ('Dazu', 'O'), ('hat', 'O'), ('zunächst', 'O'), ('das', 'O'), ('Wort', 'O'), ('Herr', 'B-EPPOL'), ('Staatssekretär', 'I-EPPOL'), ('Hartmann', 'I-EPPOL'), ('.', 'O'), ('[SEP]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'I-EPPOL'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'I-EPPOL'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O')]]\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a4b1d12f4d380db9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
