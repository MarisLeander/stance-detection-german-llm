{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ded3fa9169d5e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T14:37:38.863867Z",
     "start_time": "2025-05-30T14:37:38.860982Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from sympy.stats.rv import probability\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification, DataCollatorWithPadding\n",
    ")\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "192c407f9481815a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:39:23.798867Z",
     "start_time": "2025-05-30T15:39:23.117107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "[('Sehr', 'B-EPPOL'), ('geehrte', 'I-EPPOL'), ('Frau', 'I-EPPOL'), ('Präsidentin', 'I-EPPOL'), ('!', 'O'), ('Liebe', 'O'), ('Kollegen', 'O'), ('und', 'O'), ('Kolleginnen', 'O'), ('!', 'O'), ('Wenn', 'O'), ('wir', 'O'), ('ganz', 'O'), ('und', 'O'), ('gar', 'O'), ('aufgehört', 'O'), ('haben', 'O'), (',', 'O'), ('Kinder', 'B-PAGE'), ('zu', 'O'), ('sein', 'O'), (',', 'O'), ('dann', 'O'), ('sind', 'O'), ('wir', 'O'), ('schon', 'O'), ('tot', 'O'), ('.', 'O'), ('Das', 'O'), ('sind', 'O'), ('die', 'O'), ('Worte', 'O'), ('des', 'B-EPKULT'), ('deutschen', 'I-EPKULT'), ('Schriftstellers', 'I-EPKULT'), ('Michael', 'I-EPKULT'), ('Ende', 'I-EPKULT'), (',', 'I-EPKULT'), ('des', 'I-EPKULT'), ('Autors', 'I-EPKULT'), ('der', 'I-EPKULT'), ('Unendlichen', 'I-EPKULT'), ('Geschichte', 'I-EPKULT'), ('.', 'O'), ('Die', 'O'), ('Frage', 'O'), (',', 'O'), ('wann', 'O'), ('Kinder', 'B-PAGE'), ('wieder', 'O'), ('Kinder', 'B-PAGE'), ('sein', 'O'), ('dürfen', 'O'), (',', 'O'), ('beschäftigt', 'O'), ('mich', 'O'), ('und', 'O'), ('ist', 'O'), ('einer', 'O'), ('der', 'O'), ('Gründe', 'O'), (',', 'O'), ('warum', 'O'), ('wir', 'O'), ('hier', 'O'), ('debattieren', 'O'), ('.', 'O'), ('Denn', 'O'), ('Kinder', 'B-PAGE'), ('hatten', 'O'), ('in', 'O'), ('den', 'O'), ('vergangenen', 'O'), ('18', 'O'), ('Monaten', 'O'), ('nur', 'O'), ('sehr', 'O'), ('wenige', 'O'), ('Möglichkeiten', 'O'), (',', 'O'), ('Kind', 'B-PAGE'), ('zu', 'O'), ('sein', 'O'), ('.', 'O'), ('Kinder', 'B-PAGE'), ('wurden', 'O'), ('zu', 'O'), ('Hause', 'O'), ('isoliert', 'O'), ('.', 'O'), ('Kinder', 'B-PAGE'), ('können', 'O'), ('nicht', 'O'), ('richtig', 'O'), ('lernen', 'O'), ('.', 'O'), ('Kinder', 'B-PAGE'), ('durften', 'O'), ('nicht', 'O'), ('mehr', 'O'), ('Kind', 'O'), ('sein', 'O'), ('.', 'O'), ('Kurzum', 'O'), (':', 'O'), ('Kinder', 'B-PAGE'), ('haben', 'O'), ('seit', 'O'), ('Beginn', 'O'), ('der', 'O'), ('Pandemie', 'O'), ('gelitten', 'O'), (',', 'O'), ('und', 'O'), ('zwar', 'O'), ('sehr', 'O'), ('.', 'O')]\n",
      "Batch size: 1\n",
      "[('Ursache', 'O'), ('dafür', 'O'), ('waren', 'O'), ('vor', 'O'), ('allem', 'O'), ('die', 'O'), ('Folgen', 'O'), ('der', 'O'), ('Schul', 'O'), ('-', 'O'), ('und', 'O'), ('Kitaschließungen', 'O'), ('.', 'O'), ('Aber', 'O'), ('die', 'O'), ('flächendeckende', 'O'), ('Schließung', 'O'), ('von', 'O'), ('Schulen', 'O'), ('und', 'O'), ('Kitas', 'O'), ('wird', 'O'), ('zukünftig', 'O'), ('keine', 'O'), ('Option', 'O'), ('mehr', 'O'), ('sein', 'O'), (',', 'O'), ('auch', 'O'), ('nicht', 'O'), ('bei', 'O'), ('dramatisch', 'O'), ('hohem', 'O'), ('Infektionsgeschehen', 'O'), ('.', 'O'), ('Nur', 'O'), ('dann', 'O'), (',', 'O'), ('wenn', 'O'), ('es', 'O'), ('in', 'O'), ('einer', 'O'), ('Einrichtung', 'O'), ('ein', 'O'), ('akutes', 'O'), ('Ausbruchsgeschehen', 'O'), ('gibt', 'O'), (',', 'O'), ('welches', 'O'), ('nicht', 'O'), ('unter', 'O'), ('Kontrolle', 'O'), ('zu', 'O'), ('bekommen', 'O'), ('ist', 'O'), (',', 'O'), ('kann', 'O'), ('eine', 'O'), ('Schule', 'O'), ('oder', 'O'), ('eine', 'O'), ('Kita', 'O'), ('vorübergehend', 'O'), ('geschlossen', 'O'), ('werden', 'O'), ('.', 'O'), ('So', 'O'), ('sieht', 'O'), ('es', 'O'), ('der', 'O'), ('neue', 'O'), ('Gesetzentwurf', 'O'), ('zum', 'O'), ('Infektionsschutzgesetz', 'O'), ('vor', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def index2label(index):\n",
    "    \"\"\" Convert an index to a label.\n",
    "\n",
    "    Args:\n",
    "        index (int): The index to be converted.\n",
    "\n",
    "    Returns:\n",
    "        str: The label corresponding\n",
    "    \"\"\"\n",
    "\n",
    "    labels = {0: '[PAD]', 1: '[UNK]', 2: 'B-EGPOL', 3: 'B-EOFINANZ', 4: 'B-EOMEDIA', 5: 'B-EOMIL', 6: 'B-EOMOV', 7: 'B-EONGO', 8: 'B-EOPOL', 9: 'B-EOREL', 10: 'B-EOSCI', 11: 'B-EOWIRT', 12: 'B-EPFINANZ', 13: 'B-EPKULT', 14: 'B-EPMEDIA', 15: 'B-EPMIL', 16: 'B-EPMOV', 17: 'B-EPNGO', 18: 'B-EPPOL', 19: 'B-EPREL', 20: 'B-EPSCI', 21: 'B-EPWIRT', 22: 'B-GPE', 23: 'B-PAGE', 24: 'B-PETH', 25: 'B-PFUNK', 26: 'B-PGEN', 27: 'B-PNAT', 28: 'B-PSOZ', 29: 'I-EGPOL', 30: 'I-EOFINANZ', 31: 'I-EOMEDIA', 32: 'I-EOMIL', 33: 'I-EOMOV', 34: 'I-EONGO', 35: 'I-EOPOL', 36: 'I-EOREL', 37: 'I-EOSCI', 38: 'I-EOWIRT', 39: 'I-EPFINANZ', 40: 'I-EPKULT', 41: 'I-EPMEDIA', 42: 'I-EPMIL', 43: 'I-EPMOV', 44: 'I-EPNGO', 45: 'I-EPPOL', 46: 'I-EPREL', 47: 'I-EPSCI', 48: 'I-EPWIRT', 49: 'I-GPE', 50: 'I-PAGE', 51: 'I-PETH', 52: 'I-PFUNK', 53: 'I-PGEN', 54: 'I-PNAT', 55: 'I-PSOZ', 56: 'O'}\n",
    "\n",
    "    return labels[index]\n",
    "    #\n",
    "    # labels = [\"[PAD]\", \"[UNK]\", \"B-EGPOL\", \"B-EOFINANZ\", \"B-EOMEDIA\", \"B-EOMIL\", \"B-EOMOV\", \"B-EONGO\", \"B-EOPOL\", \"B-EOREL\", \"B-EOSCI\", \"B-EOWIRT\", \"B-EPFINANZ\", \"B-EPKULT\", \"B-EPMEDIA\", \"B-EPMIL\", \"B-EPMOV\", \"B-EPNGO\", \"B-EPPOL\", \"B-EPREL\", \"B-EPSCI\", \"B-EPWIRT\", \"B-GPE\", \"B-PAGE\", \"B-PETH\", \"B-PFUNK\", \"B-PGEN\", \"B-PNAT\", \"B-PSOZ\", \"I-EGPOL\", \"I-EOFINANZ\", \"I-EOMEDIA\", \"I-EOMIL\", \"I-EOMOV\", \"I-EONGO\", \"I-EOPOL\", \"I-EOREL\", \"I-EOSCI\", \"I-EOWIRT\", \"I-EPFINANZ\", \"I-EPKULT\", \"I-EPMEDIA\", \"I-EPMIL\", \"I-EPMOV\", \"I-EPNGO\", \"I-EPPOL\", \"I-EPREL\", \"I-EPSCI\", \"I-EPWIRT\", \"I-GPE\", \"I-PAGE\", \"I-PETH\", \"I-PFUNK\", \"I-PGEN\", \"I-PNAT\", \"I-PSOZ\", \"O\"]\n",
    "    # label2index, index2label = {}, {}\n",
    "    # for i, item in enumerate(labels):\n",
    "    #     label2index[item] = i\n",
    "    #     index2label[i] = item\n",
    "\n",
    "def load_model(model_dir):\n",
    "    \"\"\" Load a pre-trained model from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str or Path): The directory where the model is stored.\n",
    "\n",
    "    Returns:\n",
    "        model (AutoModelForTokenClassification): The loaded model.\n",
    "    \"\"\"\n",
    "    # Load the config\n",
    "    cfg   = AutoConfig.from_pretrained(model_dir)\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_dir, config=cfg).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return model\n",
    "\n",
    "def tokenize_labels(data, tokenizer):\n",
    "    \"\"\" Tokenize the input data. This is needed to prepare the input data for the model.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The input data containing the paragraphs to be tokenized.\n",
    "        tokenizer (AutoTokenizer): The tokenizer to be used for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        tokenized_inputs (dict): A dictionary containing the tokenized inputs.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(data[\"paragraphs\"],\n",
    "                                  return_tensors=\"pt\",\n",
    "                                  truncation=True,\n",
    "                                  padding=True,\n",
    "                                  is_split_into_words=False)\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "def encode_dataset(data: Dict[str, List[str]], tokenizer):\n",
    "    # data = load_dataset('json', data_files={'input_data': path}) -> Use this to import from json\n",
    "    encoded_data = data.map(\n",
    "                tokenize_labels,              # ← function reference\n",
    "                batched=True,\n",
    "                fn_kwargs={\"tokenizer\": tokenizer},  # extra objects you need\n",
    "                remove_columns=[\"paragraphs\"]# only keep the columns that are needed, i.e. input_ids, attention_mask, token_type_ids and labels. It checks if the columns are present in the corpus and removes them if they are not needed.\n",
    "            )\n",
    "    dataset = encoded_data[\"input_data\"].with_format(\"torch\")  # Convert to PyTorch format, to be compatible with DataLoader\n",
    "    return dataset\n",
    "\n",
    "def build_dataloader(dataset, tokenizer, batch_size=16):\n",
    "    # Create a DataLoader for the test dataset. The DataCollatorWithPadding will pad the sequences to the one with the maximum length in the batch, so that all sequences in the batch have the same length.\n",
    "    # The dataloader will return batches of data, which we can then pass to the model for prediction.\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "def merge_word_pieces(paired_tokens):\n",
    "    \"\"\"\n",
    "    paired_tokens : List[Tuple[str, str]]\n",
    "        e.g. [(\"Beispiel\",\"O\"), (\"##satz\",\"O\"), ...]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    merged : List[Tuple[str, str]]\n",
    "        Word-level (token, label) pairs.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    current_word = \"\"\n",
    "    current_label = None\n",
    "\n",
    "    for token, label in paired_tokens:\n",
    "        # 1. drop specials outright\n",
    "        if token in SPECIAL:\n",
    "            continue\n",
    "\n",
    "        # 2. continuation piece?\n",
    "        if token.startswith(\"##\"):\n",
    "            current_word += token[2:]        # append stem\n",
    "            continue                         # label already set\n",
    "        else:\n",
    "            # 3. flush previous buffered word\n",
    "            if current_word:\n",
    "                merged.append((current_word, current_label))\n",
    "            # 4. start new word buffer\n",
    "            current_word  = token\n",
    "            current_label = label\n",
    "\n",
    "    # flush last word\n",
    "    if current_word:\n",
    "        merged.append((current_word, current_label))\n",
    "    return merged\n",
    "\n",
    "def get_predictions(dataloader, model, tokenizer):\n",
    "\n",
    "    preds_all= [] # Initialize lists to store predictions\n",
    "\n",
    "    for batch in dataloader:\n",
    "        print(\"Batch size:\", len(batch[\"input_ids\"]))\n",
    "        with torch.inference_mode():\n",
    "            output = model(**batch) # unpacks the batch dictionary and passes the input IDs and attention mask to the model, which will return the logits, which are the unnormalized scores for each label.\n",
    "\n",
    "        logits = output.logits                    # Logits are the unnormalized scores for each label.\n",
    "        preds  = torch.argmax(logits, dim=-1)     # take the best label for each token. We use argmax for inference since we don't need to compute the loss during inference, we just want the predicted labels. Also we won't do majority voting since (now) we only use one model.\n",
    "        labels = [index2label(int(i)) for i in preds[0]]\n",
    "        tokens = tokenizer.convert_ids_to_tokens(batch[\"input_ids\"][0])\n",
    "        print(merge_word_pieces(zip(tokens,labels)))\n",
    "\n",
    "        #@ todo store the predictions in a list and compare them with the ground truth labels!!!\n",
    "\n",
    "def predict_batch(data: Dict[str, List[str]]) -> None: #@todo add return\n",
    "    model_dir = Path(\"../models/bert-base-german-cased-finetuned-MOPE-L3_Run_2_Epochs_29\")\n",
    "    tokenizer   = AutoTokenizer.from_pretrained(model_dir, use_fast=True) # use_fast=True enables the fast tokenizer implementation\n",
    "    model = load_model(model_dir)\n",
    "    encoded_dataset = encode_dataset(data, tokenizer)\n",
    "    dataloader = build_dataloader(encoded_dataset, tokenizer, batch_size=1)\n",
    "    get_predictions(dataloader, model, tokenizer)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3c3003d0255be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
