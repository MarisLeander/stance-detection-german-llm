{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d40ab22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d4f244-349a-4253-8dc7-1a4741390ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab6cc9-aeec-4ed8-bfad-13d27c9ad353",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Tell vLLM to use 2 GPUs for tensor parallelism.\n",
    "# This automatically splits the model across the 2 A100s.\n",
    "# The process takes about 30min!\n",
    "llm = llm = LLM(model=model_name, tensor_parallel_size=2)\n",
    "\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print(f\"LLM setup took {elapsed_time} min.\")\n",
    "\n",
    "# Load the tokenizer for the same model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ee2406-50e8-4f23-a8c9-37ef90dec759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for the same model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e5f1ef3-8318-472f-9478-bd09a2fd91ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9e67172f2c4d41b803cf5a1fe517fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e23f84bb244f00a7c4758734f589d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Input to Model ---\n",
      "'<｜begin▁of▁sentence｜>You are a friendly and helpful AI chatbot. Your goal is to assist the user with their questions in a clear and concise way.<｜User｜>Bitte gib die Speaker stance für den folgenden Text aus den Labeln [Favour, Against, Neither] an. Target: \"der luxemburgischen Ratspräsidentschaft in\" (markiert durch <span> tag). Text: Die Einigung ist für uns deshalb mindestens so entscheidend wie für die anderen europäischen Staaten. Im Juni haben wir die Einigung unter <span>der luxemburgischen Ratspräsidentschaft in</span> Luxemburg schon einmal versucht. Ich sage voraus: Wenn wir am Ende dieses Jahres mit dem zweiten Versuch einer Einigung über den Finanzrahmen erneut scheitern würden, dann ginge davon ein verheerendes Signal für die Bürgerinnen und Bürger aus. Insbesondere darf nicht vergessen werden, dass sich ein Scheitern vor allem zulasten der neuen Mitgliedstaaten auswirken würde.<｜Assistant｜><think>\\n'\n",
      "\n",
      "--- Generated Output ---\n",
      "'Okay, I need to determine the speaker\\'s stance towards the target phrase \"der luxemburgischen Ratspräsidentschaft in\" from the text. The options are Favour, Against, or Neither. \\n\\nFirst, I\\'ll read the text carefully. The speaker talks about an agreement being crucial for all European states. They mention that under the Luxembourgish presidency, they tried to reach an agreement in June. They predict that failing again would send a terrible signal, especially affecting new member states.\\n\\nLooking at the target phrase, it\\'s part of a statement about attempting an agreement. The speaker doesn\\'t express a positive or negative opinion about the presidency itself. They\\'re just stating a fact about the attempt. So, the stance isn\\'t clearly for or against; it\\'s more neutral.\\n\\nTherefore, the stance should be Neither.\\n</think>\\n\\nNeither'\n"
     ]
    }
   ],
   "source": [
    "# Define the conversation roles\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly and helpful AI chatbot. Your goal is to assist the user with their questions in a clear and concise way.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Bitte gib die Speaker stance für den folgenden Text aus den Labeln [Favour, Against, Neither] an. Target: \\\"der luxemburgischen Ratspräsidentschaft in\\\" (markiert durch <span> tag). Text: Die Einigung ist für uns deshalb mindestens so entscheidend wie für die anderen europäischen Staaten. Im Juni haben wir die Einigung unter <span>der luxemburgischen Ratspräsidentschaft in</span> Luxemburg schon einmal versucht. Ich sage voraus: Wenn wir am Ende dieses Jahres mit dem zweiten Versuch einer Einigung über den Finanzrahmen erneut scheitern würden, dann ginge davon ein verheerendes Signal für die Bürgerinnen und Bürger aus. Insbesondere darf nicht vergessen werden, dass sich ein Scheitern vor allem zulasten der neuen Mitgliedstaaten auswirken würde.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Applying the chat template to create the final prompt string.\n",
    "#    - `tokenize=False` makes it return a string, which is what llm.generate expects.\n",
    "#    - `add_generation_prompt=True` adds the special tokens to signal to the model that it's the assistant's turn to speak.\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "# Single formatted prompt for model\n",
    "prompts_to_generate = [formatted_prompt]\n",
    "\n",
    "\n",
    "# sampling parameters for generation.\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=1000 # Adjust max_tokens as needed\n",
    ")\n",
    "\n",
    "# Generate completions for all prompts in a batch using vllm\n",
    "\n",
    "outputs = llm.generate(prompts_to_generate, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    # The 'prompt' here will be the long, formatted string with special tokens\n",
    "    original_prompt_info = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    \n",
    "    print(\"--- Input to Model ---\")\n",
    "    print(f\"{original_prompt_info!r}\")\n",
    "    print(\"\\n--- Generated Output ---\")\n",
    "    print(f\"{generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc25be6-275f-4aca-ab3a-595f914f1dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
